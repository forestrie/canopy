name: Performance Tests

on:
  # Run after successful deploy-workers completion
  workflow_run:
    workflows: ["Deploy Workers"]
    types:
      - completed
    branches: [main]
  # Allow manual trigger with rate selection
  workflow_dispatch:
    inputs:
      rate:
        description: "Request rate (req/s)"
        required: false
        default: "300"
        type: choice
        options:
          - "10"
          - "25"
          - "50"
          - "100"
          - "110"
          - "150"
          - "250"
          - "300"
          - "400"
          - "800"
          - "1000"
      duration:
        description: "Test duration"
        required: false
        default: "3m"
        type: string
      warmup:
        description: "Warmup duration"
        required: false
        default: "30s"
        type: string
      log_count:
        description: "Number of distinct logs to distribute requests across"
        required: false
        default: "1"
        type: choice
        options:
          - "1"
          - "2"
          - "4"
          - "8"
          - "10"

permissions:
  contents: read

env:
  CANOPY_PERF_BASE_URL: https://canopy-api.robinbryce.workers.dev
  # forestrie-ingress URL for queue stats (same worker, different route)
  FORESTRIE_INGRESS_URL: https://forestrie-ingress.robinbryce.workers.dev
  # Default log IDs (pre-generated UUIDs for reproducibility)
  CANOPY_PERF_LOG_ID_0: 3062ea57-c184-41d8-bd61-296b02c680d8
  CANOPY_PERF_LOG_ID_1: a1b2c3d4-e5f6-4a7b-8c9d-0e1f2a3b4c5d
  CANOPY_PERF_LOG_ID_2: b2c3d4e5-f6a7-4b8c-9d0e-1f2a3b4c5d6e
  CANOPY_PERF_LOG_ID_3: c3d4e5f6-a7b8-4c9d-0e1f-2a3b4c5d6e7f
  CANOPY_PERF_LOG_ID_4: d4e5f6a7-b8c9-4d0e-1f2a-3b4c5d6e7f8a
  CANOPY_PERF_LOG_ID_5: e5f6a7b8-c9d0-4e1f-2a3b-4c5d6e7f8a9b
  CANOPY_PERF_LOG_ID_6: f6a7b8c9-d0e1-4f2a-3b4c-5d6e7f8a9b0c
  CANOPY_PERF_LOG_ID_7: a7b8c9d0-e1f2-4a3b-4c5d-6e7f8a9b0c1d
  CANOPY_PERF_LOG_ID_8: b8c9d0e1-f2a3-4b4c-5d6e-7f8a9b0c1d2e
  CANOPY_PERF_LOG_ID_9: c9d0e1f2-a3b4-4c5d-6e7f-8a9b0c1d2e3f

jobs:
  perf-test:
    name: k6 Performance Test (${{ matrix.rate }} req/s)
    runs-on: ubuntu-latest
    # Only run if deploy-workers succeeded (for workflow_run trigger)
    # Always run for workflow_dispatch
    if: >
      github.event_name == 'workflow_dispatch' ||
      github.event.workflow_run.conclusion == 'success'
    strategy:
      fail-fast: false
      matrix:
        rate:
          - ${{ github.event.inputs.rate || '10' }}

    env:
      CANOPY_PERF_API_TOKEN: ${{ secrets.CANOPY_PERF_API_TOKEN }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup k6
        uses: grafana/setup-k6-action@v1

      - name: Build log ID list and compute shard distribution
        id: log_ids
        run: |
          LOG_COUNT=${{ github.event.inputs.log_count || '1' }}
          echo "log_count=$LOG_COUNT" >> $GITHUB_OUTPUT

          # djb2 hash function (matches @canopy/forestrie-sharding)
          djb2_hash() {
            local str="$1"
            local hash=5381
            for (( i=0; i<${#str}; i++ )); do
              char=$(printf '%d' "'${str:$i:1}")
              # hash = ((hash << 5) + hash) + char = hash * 33 + char
              hash=$(( ((hash << 5) + hash + char) & 0xFFFFFFFF ))
            done
            # Convert to unsigned 32-bit
            echo $(( hash & 0x7FFFFFFF ))
          }

          # Build comma-separated list of log IDs from defaults
          LOG_IDS=""
          declare -a SHARD_LOGS  # Array to track logs per shard
          for i in $(seq 0 $((LOG_COUNT - 1))); do
            case $i in
              0) ID="$CANOPY_PERF_LOG_ID_0" ;;
              1) ID="$CANOPY_PERF_LOG_ID_1" ;;
              2) ID="$CANOPY_PERF_LOG_ID_2" ;;
              3) ID="$CANOPY_PERF_LOG_ID_3" ;;
              4) ID="$CANOPY_PERF_LOG_ID_4" ;;
              5) ID="$CANOPY_PERF_LOG_ID_5" ;;
              6) ID="$CANOPY_PERF_LOG_ID_6" ;;
              7) ID="$CANOPY_PERF_LOG_ID_7" ;;
              8) ID="$CANOPY_PERF_LOG_ID_8" ;;
              9) ID="$CANOPY_PERF_LOG_ID_9" ;;
            esac

            if [ -z "$LOG_IDS" ]; then
              LOG_IDS="$ID"
            else
              LOG_IDS="$LOG_IDS,$ID"
            fi
          done

          echo "log_ids=$LOG_IDS" >> $GITHUB_OUTPUT
          echo "Using $LOG_COUNT log(s): $LOG_IDS"

          # Compute shard distribution (will be updated after we know shard count)
          # Save log IDs to file for later processing
          echo "$LOG_IDS" > /tmp/log_ids.txt

      - name: Capture queue stats (before)
        id: stats_before
        run: |
          echo "Fetching queue stats before test..."
          STATS=$(curl -s -H "Authorization: Bearer $CANOPY_PERF_API_TOKEN" \
            "${FORESTRIE_INGRESS_URL}/queue/stats")
          echo "stats=$STATS" >> $GITHUB_OUTPUT
          PENDING=$(echo "$STATS" | jq -r '.pending // 0')
          SHARD_COUNT=$(echo "$STATS" | jq -r '.shardCount // 1')
          echo "pending=$PENDING" >> $GITHUB_OUTPUT
          echo "shard_count=$SHARD_COUNT" >> $GITHUB_OUTPUT
          echo "Queue pending before: $PENDING (across $SHARD_COUNT shards)"
          echo "$STATS" | jq -c '.perShard[]? | {shard: .index, pending: .pending, pollers: .activePollers}' 2>/dev/null || true
          date +%s > /tmp/test_start_time

          # Compute and display log-to-shard distribution
          if [ -f /tmp/log_ids.txt ] && [ "$SHARD_COUNT" -gt 0 ]; then
            echo ""
            echo "Log-to-shard distribution (djb2 hash % $SHARD_COUNT):"

            # djb2 hash function (matches @canopy/forestrie-sharding)
            djb2_hash() {
              local str="$1"
              local hash=5381
              for (( i=0; i<${#str}; i++ )); do
                char=$(printf '%d' "'${str:$i:1}")
                hash=$(( ((hash << 5) + hash + char) & 0xFFFFFFFF ))
              done
              echo $(( hash & 0x7FFFFFFF ))
            }

            # Initialize shard counts
            declare -a SHARD_COUNTS
            for (( s=0; s<SHARD_COUNT; s++ )); do
              SHARD_COUNTS[$s]=0
            done

            # Process each log ID
            LOG_IDS=$(cat /tmp/log_ids.txt)
            IFS=',' read -ra IDS <<< "$LOG_IDS"
            DISTRIBUTION=""
            for log_id in "${IDS[@]}"; do
              hash=$(djb2_hash "$log_id")
              shard=$((hash % SHARD_COUNT))
              SHARD_COUNTS[$shard]=$((SHARD_COUNTS[$shard] + 1))
              echo "  $log_id -> shard $shard (hash=$hash)"
              DISTRIBUTION="$DISTRIBUTION$log_id:$shard,"
            done

            # Show summary
            echo ""
            echo "Logs per shard:"
            for (( s=0; s<SHARD_COUNT; s++ )); do
              echo "  shard $s: ${SHARD_COUNTS[$s]} log(s)"
            done

            # Save distribution for summary
            echo "$DISTRIBUTION" > /tmp/log_shard_distribution.txt
          fi

      - name: Run k6 performance test
        id: k6
        env:
          CANOPY_PERF_RATE: ${{ matrix.rate }}
          CANOPY_PERF_DURATION: ${{ github.event.inputs.duration || '3m' }}
          CANOPY_PERF_WARMUP: ${{ github.event.inputs.warmup || '30s' }}
          CANOPY_PERF_SAMPLE_RATE: "0.05"
          CANOPY_PERF_LOG_IDS: ${{ steps.log_ids.outputs.log_ids }}
          CANOPY_PERF_LOG_COUNT: ${{ steps.log_ids.outputs.log_count }}
        run: |
          echo "Running k6 performance test"
          echo "  Rate: ${CANOPY_PERF_RATE} req/s"
          echo "  Duration: ${CANOPY_PERF_DURATION}"
          echo "  Warmup: ${CANOPY_PERF_WARMUP}"
          echo "  Sample rate: ${CANOPY_PERF_SAMPLE_RATE} (for e2e latency)"
          echo "  Log count: ${CANOPY_PERF_LOG_COUNT}"
          echo "  Log IDs: ${CANOPY_PERF_LOG_IDS}"
          echo "  Target: ${CANOPY_PERF_BASE_URL}/logs/.../entries"
          k6 run --no-usage-report perf/k6/canopy-api/scenarios/write-constant-arrival.js

      - name: Capture queue stats (after)
        if: always()
        id: stats_after
        run: |
          echo "Fetching queue stats after test..."
          STATS=$(curl -s -H "Authorization: Bearer $CANOPY_PERF_API_TOKEN" \
            "${FORESTRIE_INGRESS_URL}/queue/stats")
          echo "stats=$STATS" >> $GITHUB_OUTPUT
          PENDING=$(echo "$STATS" | jq -r '.pending // 0')
          echo "pending=$PENDING" >> $GITHUB_OUTPUT
          echo "Queue pending after: $PENDING"
          # Save full stats for per-shard analysis
          echo "$STATS" > /tmp/stats_after.json
          echo "Per-shard breakdown:"
          echo "$STATS" | jq -c '.perShard[]? | {shard: .index, pending: .pending, pollers: .activePollers}' 2>/dev/null || true
          date +%s > /tmp/test_end_time

      - name: Generate job summary
        if: always()
        env:
          PENDING_BEFORE: ${{ steps.stats_before.outputs.pending }}
          PENDING_AFTER: ${{ steps.stats_after.outputs.pending }}
          SHARD_COUNT: ${{ steps.stats_before.outputs.shard_count }}
        run: |
          if [ -f summary.json ]; then
            echo "## üìä k6 Performance Test Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Extract config
            BASE_URL=$(jq -r '.config.baseUrl' summary.json)
            LOG_COUNT=$(jq -r '.config.logCount // 1' summary.json)
            LOG_IDS=$(jq -r '.config.logIds // []' summary.json)
            RATE=$(jq -r '.config.rate' summary.json)
            DURATION=$(jq -r '.config.duration' summary.json)
            WARMUP=$(jq -r '.config.warmup' summary.json)

            echo "### Configuration" >> $GITHUB_STEP_SUMMARY
            echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Target URL | \`${BASE_URL}/logs/.../entries\` |" >> $GITHUB_STEP_SUMMARY
            echo "| Log count | ${LOG_COUNT} |" >> $GITHUB_STEP_SUMMARY
            echo "| Shard count | ${SHARD_COUNT:-1} |" >> $GITHUB_STEP_SUMMARY
            echo "| Target Rate | ${RATE} req/s |" >> $GITHUB_STEP_SUMMARY
            echo "| Duration | ${WARMUP} warmup + ${DURATION} sustained |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Log-to-shard distribution (if multiple shards)
            if [ -f /tmp/log_shard_distribution.txt ] && [ "${SHARD_COUNT:-1}" -gt 1 ]; then
              echo "### Log-to-Shard Distribution" >> $GITHUB_STEP_SUMMARY
              echo "| Log ID | Shard |" >> $GITHUB_STEP_SUMMARY
              echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY

              # Parse distribution file (format: logid:shard,logid:shard,...)
              DIST=$(cat /tmp/log_shard_distribution.txt)
              IFS=',' read -ra PAIRS <<< "$DIST"
              declare -A SHARD_LOG_COUNTS
              for pair in "${PAIRS[@]}"; do
                if [ -n "$pair" ]; then
                  log_id=$(echo "$pair" | cut -d: -f1)
                  shard=$(echo "$pair" | cut -d: -f2)
                  echo "| \`$log_id\` | $shard |" >> $GITHUB_STEP_SUMMARY
                  SHARD_LOG_COUNTS[$shard]=$((${SHARD_LOG_COUNTS[$shard]:-0} + 1))
                fi
              done
              echo "" >> $GITHUB_STEP_SUMMARY

              # Show distribution summary
              echo "**Logs per shard:**" >> $GITHUB_STEP_SUMMARY
              for shard in $(echo "${!SHARD_LOG_COUNTS[@]}" | tr ' ' '\n' | sort -n); do
                echo "- Shard $shard: ${SHARD_LOG_COUNTS[$shard]} log(s)" >> $GITHUB_STEP_SUMMARY
              done

              # Check for imbalance
              TOTAL_LOGS=${LOG_COUNT}
              IDEAL_PER_SHARD=$((TOTAL_LOGS / SHARD_COUNT))
              MAX_LOGS=0
              MIN_LOGS=${TOTAL_LOGS}
              for count in "${SHARD_LOG_COUNTS[@]}"; do
                [ "$count" -gt "$MAX_LOGS" ] && MAX_LOGS=$count
                [ "$count" -lt "$MIN_LOGS" ] && MIN_LOGS=$count
              done
              if [ "$MAX_LOGS" -gt $((IDEAL_PER_SHARD + 1)) ] || [ "$MIN_LOGS" -lt $((IDEAL_PER_SHARD - 1)) ]; then
                echo "" >> $GITHUB_STEP_SUMMARY
                echo "> ‚ö†Ô∏è **Uneven distribution**: Some shards have more logs than others. Consider using different log IDs for better balance." >> $GITHUB_STEP_SUMMARY
              fi
              echo "" >> $GITHUB_STEP_SUMMARY
            fi

            # Extract metrics
            HTTP_COUNT=$(jq -r '.metrics.http_reqs.count // "N/A"' summary.json)
            HTTP_RATE=$(jq -r '.metrics.http_reqs.rate // "N/A"' summary.json)
            ERRORS=$(jq -r '.metrics.post_errors.count // 0' summary.json)

            echo "### Throughput" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Total Requests | ${HTTP_COUNT} |" >> $GITHUB_STEP_SUMMARY
            if [ "$HTTP_RATE" != "N/A" ]; then
              HTTP_RATE_FMT=$(printf "%.2f" "$HTTP_RATE")
              echo "| Achieved Rate | ${HTTP_RATE_FMT} req/s |" >> $GITHUB_STEP_SUMMARY
            fi
            echo "| Errors | ${ERRORS} |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Extract latency
            AVG=$(jq -r '.metrics.post_latency.avg // "N/A"' summary.json)
            MED=$(jq -r '.metrics.post_latency.med // "N/A"' summary.json)
            P90=$(jq -r '.metrics.post_latency.p90 // "N/A"' summary.json)
            P95=$(jq -r '.metrics.post_latency.p95 // "N/A"' summary.json)
            P99=$(jq -r '.metrics.post_latency.p99 // "N/A"' summary.json)
            MAX=$(jq -r '.metrics.post_latency.max // "N/A"' summary.json)

            echo "### POST Latency (time to 303)" >> $GITHUB_STEP_SUMMARY
            echo "| Percentile | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|------------|-------|" >> $GITHUB_STEP_SUMMARY
            if [ "$AVG" != "N/A" ]; then
              AVG_FMT=$(printf "%.0f" "$AVG")
              echo "| avg | ${AVG_FMT} ms |" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "$MED" != "N/A" ]; then
              MED_FMT=$(printf "%.0f" "$MED")
              echo "| p50 (median) | ${MED_FMT} ms |" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "$P90" != "N/A" ]; then
              P90_FMT=$(printf "%.0f" "$P90")
              echo "| p90 | ${P90_FMT} ms |" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "$P95" != "N/A" ]; then
              P95_FMT=$(printf "%.0f" "$P95")
              echo "| p95 | ${P95_FMT} ms |" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "$P99" != "N/A" ]; then
              P99_FMT=$(printf "%.0f" "$P99")
              echo "| p99 | ${P99_FMT} ms |" >> $GITHUB_STEP_SUMMARY
            fi
            if [ "$MAX" != "N/A" ]; then
              MAX_FMT=$(printf "%.0f" "$MAX")
              echo "| max | ${MAX_FMT} ms |" >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY

            # Extract e2e latency (sampled)
            E2E_COUNT=$(jq -r '.metrics.e2e_latency.count // 0' summary.json)
            if [ "$E2E_COUNT" != "0" ] && [ "$E2E_COUNT" != "null" ]; then
              E2E_AVG=$(jq -r '.metrics.e2e_latency.avg // "N/A"' summary.json)
              E2E_MED=$(jq -r '.metrics.e2e_latency.med // "N/A"' summary.json)
              E2E_P95=$(jq -r '.metrics.e2e_latency.p95 // "N/A"' summary.json)
              E2E_P99=$(jq -r '.metrics.e2e_latency.p99 // "N/A"' summary.json)
              E2E_MAX=$(jq -r '.metrics.e2e_latency.max // "N/A"' summary.json)
              E2E_TIMEOUTS=$(jq -r '.metrics.e2e_timeout_count // 0' summary.json)

              echo "### E2E Latency (POST to sequenced, ${E2E_COUNT} samples)" >> $GITHUB_STEP_SUMMARY
              echo "| Percentile | Value |" >> $GITHUB_STEP_SUMMARY
              echo "|------------|-------|" >> $GITHUB_STEP_SUMMARY
              if [ "$E2E_AVG" != "N/A" ]; then
                E2E_AVG_FMT=$(printf "%.0f" "$E2E_AVG")
                echo "| avg | ${E2E_AVG_FMT} ms |" >> $GITHUB_STEP_SUMMARY
              fi
              if [ "$E2E_MED" != "N/A" ]; then
                E2E_MED_FMT=$(printf "%.0f" "$E2E_MED")
                echo "| p50 (median) | ${E2E_MED_FMT} ms |" >> $GITHUB_STEP_SUMMARY
              fi
              if [ "$E2E_P95" != "N/A" ]; then
                E2E_P95_FMT=$(printf "%.0f" "$E2E_P95")
                echo "| p95 | ${E2E_P95_FMT} ms |" >> $GITHUB_STEP_SUMMARY
              fi
              if [ "$E2E_P99" != "N/A" ]; then
                E2E_P99_FMT=$(printf "%.0f" "$E2E_P99")
                echo "| p99 | ${E2E_P99_FMT} ms |" >> $GITHUB_STEP_SUMMARY
              fi
              if [ "$E2E_MAX" != "N/A" ]; then
                E2E_MAX_FMT=$(printf "%.0f" "$E2E_MAX")
                echo "| max | ${E2E_MAX_FMT} ms |" >> $GITHUB_STEP_SUMMARY
              fi
              if [ "$E2E_TIMEOUTS" != "0" ] && [ "$E2E_TIMEOUTS" != "null" ]; then
                echo "| ‚ö†Ô∏è timeouts | ${E2E_TIMEOUTS} |" >> $GITHUB_STEP_SUMMARY
              fi
              echo "" >> $GITHUB_STEP_SUMMARY
            fi

            # Sequencing throughput (calculated from queue stats)
            if [ -f /tmp/test_start_time ] && [ -f /tmp/test_end_time ]; then
              START_TIME=$(cat /tmp/test_start_time)
              END_TIME=$(cat /tmp/test_end_time)
              TEST_DURATION=$((END_TIME - START_TIME))

              # Get pending counts from env (set by step outputs)
              QUEUE_BEFORE=${PENDING_BEFORE:-0}
              QUEUE_AFTER=${PENDING_AFTER:-0}

              if [ "$TEST_DURATION" -gt 0 ] && [ "$HTTP_COUNT" != "N/A" ]; then
                # Backlog growth = after - before
                BACKLOG_GROWTH=$((QUEUE_AFTER - QUEUE_BEFORE))

                # Sequenced = total requests - backlog growth
                # (entries that went in and came out during the test)
                HTTP_COUNT_INT=$(printf "%.0f" "$HTTP_COUNT")
                SEQUENCED=$((HTTP_COUNT_INT - BACKLOG_GROWTH))

                # Rates
                SEQ_RATE=$(echo "scale=2; $SEQUENCED / $TEST_DURATION" | bc)
                BACKLOG_RATE=$(echo "scale=2; $BACKLOG_GROWTH / $TEST_DURATION" | bc)

                echo "### Sequencing Throughput (from queue stats)" >> $GITHUB_STEP_SUMMARY
                echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
                echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
                echo "| Queue pending (before) | ${QUEUE_BEFORE} |" >> $GITHUB_STEP_SUMMARY
                echo "| Queue pending (after) | ${QUEUE_AFTER} |" >> $GITHUB_STEP_SUMMARY
                echo "| Backlog growth | ${BACKLOG_GROWTH} |" >> $GITHUB_STEP_SUMMARY
                echo "| Test duration | ${TEST_DURATION}s |" >> $GITHUB_STEP_SUMMARY
                echo "| **Sequencing rate** | **${SEQ_RATE} entries/s** |" >> $GITHUB_STEP_SUMMARY
                echo "| Backlog growth rate | ${BACKLOG_RATE} entries/s |" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY

                # Add interpretation
                if [ "$BACKLOG_GROWTH" -gt 0 ]; then
                  echo "> ‚ö†Ô∏è **Queue building backlog**: Ingress rate exceeds sequencing capacity" >> $GITHUB_STEP_SUMMARY
                elif [ "$BACKLOG_GROWTH" -lt 0 ]; then
                  echo "> ‚úÖ **Queue draining**: Sequencing keeping up with ingress" >> $GITHUB_STEP_SUMMARY
                else
                  echo "> ‚úÖ **Queue stable**: Sequencing matches ingress rate" >> $GITHUB_STEP_SUMMARY
                fi
                echo "" >> $GITHUB_STEP_SUMMARY
              fi
            fi

            # Per-shard breakdown (if stats file exists)
            if [ -f /tmp/stats_after.json ]; then
              SHARD_COUNT_VAL=$(jq -r '.shardCount // 0' /tmp/stats_after.json)
              if [ "$SHARD_COUNT_VAL" -gt 1 ]; then
                echo "### Per-Shard Queue Status (after test)" >> $GITHUB_STEP_SUMMARY
                echo "| Shard | Pending | Active Pollers |" >> $GITHUB_STEP_SUMMARY
                echo "|-------|---------|----------------|" >> $GITHUB_STEP_SUMMARY
                jq -r '.perShard[]? | "| \(.index) | \(.pending) | \(.activePollers) |"' /tmp/stats_after.json >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
              fi
            fi

            # Thresholds
            echo "### Thresholds" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Check each threshold
            POST_LATENCY_OK=$(jq -r '.thresholds.post_latency["p(99)<5000"] // false' summary.json)
            HTTP_DURATION_P95_OK=$(jq -r '.thresholds.http_req_duration["p(95)<3000"] // false' summary.json)
            HTTP_DURATION_P99_OK=$(jq -r '.thresholds.http_req_duration["p(99)<5000"] // false' summary.json)
            HTTP_FAILED_OK=$(jq -r '.thresholds.http_req_failed["rate<0.01"] // false' summary.json)

            if [ "$POST_LATENCY_OK" = "true" ]; then
              echo "- ‚úÖ POST latency p99 < 5000ms" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ‚ùå POST latency p99 < 5000ms" >> $GITHUB_STEP_SUMMARY
            fi

            if [ "$HTTP_DURATION_P95_OK" = "true" ]; then
              echo "- ‚úÖ HTTP duration p95 < 3000ms" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ‚ùå HTTP duration p95 < 3000ms" >> $GITHUB_STEP_SUMMARY
            fi

            if [ "$HTTP_DURATION_P99_OK" = "true" ]; then
              echo "- ‚úÖ HTTP duration p99 < 5000ms" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ‚ùå HTTP duration p99 < 5000ms" >> $GITHUB_STEP_SUMMARY
            fi

            if [ "$HTTP_FAILED_OK" = "true" ]; then
              echo "- ‚úÖ HTTP failure rate < 1%" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ‚ùå HTTP failure rate < 1%" >> $GITHUB_STEP_SUMMARY
            fi

            echo "" >> $GITHUB_STEP_SUMMARY
            echo "_Generated at $(jq -r '.timestamp' summary.json)_" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ö†Ô∏è Performance Test" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "No summary.json file found. Test may have failed before completion." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload summary artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: k6-summary-${{ matrix.rate }}rps
          path: summary.json
          retention-days: 30
          if-no-files-found: ignore
